
[initialize]
vectors = "en_core_web_lg"

Pretrain the “token to vector” (Tok2vec) layer of pipeline components on raw text, using an approximate
language-modeling objective. Specifically, we load pretrained vectors, and train a component like a CNN,
BiLSTM, etc to predict vectors which match the pretrained ones. The weights are saved to a directory after each epoch. 
You can then include a path to one of these pretrained weights files in your training config as the init_tok2vec 
setting when you train your pipeline. This technique may be especially helpful if you have little labelled data. 
See the usage docs on pretraining for more info. To read the raw text, a JsonlCorpus is typically used.