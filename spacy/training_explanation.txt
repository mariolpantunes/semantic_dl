These pretraining objectives use a trick that we term language modelling with approximate outputs (LMAO). 
The motivation for the trick is that predicting an exact word ID introduces a lot of incidental complexity.
You need a large output layer, and even then, the vocabulary is too large, which motivates tokenization schemes that do not align to actual word boundaries.
At the end of training, the output layer will be thrown away regardless: we just want a task that forces the network to model something about word cooccurrence statistics. 
Predicting leading and trailing characters does that more than adequately, as the exact word sequence could be recovered with 
high accuracy if the initial and trailing characters are predicted accurately. With the vectors objective, the pretraining uses the 
embedding space learned by an algorithm such as GloVe or Word2vec, allowing the model to focus on the contextual modelling we actual care about.

[initialize]
vectors = "en_core_web_lg"

Pretrain the “token to vector” (Tok2vec) layer of pipeline components on raw text, using an approximate
language-modeling objective. Specifically, we load pretrained vectors, and train a component like a CNN,
BiLSTM, etc to predict vectors which match the pretrained ones. The weights are saved to a directory after each epoch. 
You can then include a path to one of these pretrained weights files in your training config as the init_tok2vec 
setting when you train your pipeline. This technique may be especially helpful if you have little labelled data. 
See the usage docs on pretraining for more info. To read the raw text, a JsonlCorpus is typically used.