These pretraining objectives use a trick that we term language modelling with approximate outputs (LMAO). 
The motivation for the trick is that predicting an exact word ID introduces a lot of incidental complexity.
You need a large output layer, and even then, the vocabulary is too large, which motivates tokenization schemes that do not align to actual word boundaries.
At the end of training, the output layer will be thrown away regardless: we just want a task that forces the network to model something about word cooccurrence statistics. 
Predicting leading and trailing characters does that more than adequately, as the exact word sequence could be recovered with 
high accuracy if the initial and trailing characters are predicted accurately. With the vectors objective, the pretraining uses the 
embedding space learned by an algorithm such as GloVe or Word2vec, allowing the model to focus on the contextual modelling we actual care about.